{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPZQm4eIkJt1Ju4cCHqXg2L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mulcek04/DRL_Optimization_Public/blob/main/DRL_for_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcwmxkWH-FNY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe0ba73-d533-4224-99c1-db42af7d7ecf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "123\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device = torch.device('cpu')\n",
        "print(123)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"Encodes the static & dynamic states using 1d Convolution.\"\"\"\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv = nn.Conv1d(input_size, hidden_size, kernel_size=1)\n",
        "    def forward(self, input):\n",
        "        output = self.conv(input)\n",
        "        return output  # (batch, hidden_size, seq_len)\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    \"\"\"Calculates attention over the input nodes given the current state.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        # W processes features from static decoder elements\n",
        "        self.v = nn.Parameter(torch.zeros((1, 1, hidden_size),\n",
        "                                          device=device, requires_grad=True))\n",
        "        self.W = nn.Parameter(torch.zeros((1, hidden_size, 3 * hidden_size),\n",
        "                                          device=device, requires_grad=True))\n",
        "\n",
        "    def forward(self, static_hidden, dynamic_hidden, decoder_hidden):\n",
        "\n",
        "        batch_size, hidden_size, _ = static_hidden.size()\n",
        "\n",
        "        hidden = decoder_hidden.unsqueeze(2).expand_as(static_hidden)\n",
        "        hidden = torch.cat((static_hidden, dynamic_hidden, hidden), 1)\n",
        "\n",
        "        # Broadcast some dimensions so we can do batch-matrix-multiply\n",
        "        v = self.v.expand(batch_size, 1, hidden_size)\n",
        "        W = self.W.expand(batch_size, hidden_size, -1)\n",
        "\n",
        "        attns = torch.bmm(v, torch.tanh(torch.bmm(W, hidden)))\n",
        "        attns = F.softmax(attns, dim=2)  # (batch, seq_len)\n",
        "        return attns\n",
        "\n",
        "\n",
        "class Pointer(nn.Module):\n",
        "    \"\"\"Calculates the next state given the previous state and input embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, num_layers=1, dropout=0.2):\n",
        "        super(Pointer, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Used to calculate probability of selecting next state\n",
        "        self.v = nn.Parameter(torch.zeros((1, 1, hidden_size),\n",
        "                                          device=device, requires_grad=True))\n",
        "\n",
        "        self.W = nn.Parameter(torch.zeros((1, hidden_size, 2 * hidden_size),\n",
        "                                          device=device, requires_grad=True))\n",
        "\n",
        "        # Used to compute a representation of the current decoder output\n",
        "        # GRU（输入dim，隐含层dim，层数）\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, num_layers,\n",
        "                          batch_first=True,\n",
        "                          dropout=dropout if num_layers > 1 else 0)\n",
        "        self.encoder_attn = Attention(hidden_size)\n",
        "\n",
        "        self.drop_rnn = nn.Dropout(p=dropout)\n",
        "        self.drop_hh = nn.Dropout(p=dropout)\n",
        "\n",
        "    def forward(self, static_hidden, dynamic_hidden, decoder_hidden, last_hh):\n",
        "\n",
        "        rnn_out, last_hh = self.gru(decoder_hidden.transpose(2, 1), last_hh)\n",
        "        rnn_out = rnn_out.squeeze(1)\n",
        "\n",
        "        # Always apply dropout on the RNN output\n",
        "        rnn_out = self.drop_rnn(rnn_out)\n",
        "        if self.num_layers == 1:\n",
        "            # If > 1 layer dropout is already applied\n",
        "            last_hh = self.drop_hh(last_hh)\n",
        "\n",
        "        # Given a summary of the output, find an  input context\n",
        "        enc_attn = self.encoder_attn(static_hidden, dynamic_hidden, rnn_out)\n",
        "        context = enc_attn.bmm(static_hidden.permute(0, 2, 1))  # (B, 1, num_feats)\n",
        "\n",
        "        # Calculate the next output using Batch-matrix-multiply ops\n",
        "        context = context.transpose(1, 2).expand_as(static_hidden)\n",
        "        energy = torch.cat((static_hidden, context), dim=1)  # (B, num_feats, seq_len)\n",
        "\n",
        "        v = self.v.expand(static_hidden.size(0), -1, -1)\n",
        "        W = self.W.expand(static_hidden.size(0), -1, -1)\n",
        "\n",
        "        probs = torch.bmm(v, torch.tanh(torch.bmm(W, energy))).squeeze(1)\n",
        "\n",
        "        return probs, last_hh\n",
        "\n",
        "\n",
        "class DRL4TSP(nn.Module):\n",
        "    \"\"\"Defines the main Encoder, Decoder, and Pointer combinatorial models.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    static_size: int\n",
        "        Defines how many features are in the static elements of the model\n",
        "        (e.g. 2 for (x, y) coordinates)\n",
        "    dynamic_size: int > 1\n",
        "        Defines how many features are in the dynamic elements of the model\n",
        "        (e.g. 2 for the VRP which has (load, demand) attributes. The TSP doesn't\n",
        "        have dynamic elements, but to ensure compatility with other optimization\n",
        "        problems, assume we just pass in a vector of zeros.\n",
        "    hidden_size: int\n",
        "        Defines the number of units in the hidden layer for all static, dynamic,\n",
        "        and decoder output units.\n",
        "    update_fn: function or None\n",
        "        If provided, this method is used to calculate how the input dynamic\n",
        "        elements are updated, and is called after each 'point' to the input element.\n",
        "    mask_fn: function or None\n",
        "        Allows us to specify which elements of the input sequence are allowed to\n",
        "        be selected. This is useful for speeding up training of the networks,\n",
        "        by providing a sort of 'rules' guidlines to the algorithm. If no mask\n",
        "        is provided, we terminate the search after a fixed number of iterations\n",
        "        to avoid tours that stretch forever\n",
        "    num_layers: int\n",
        "        Specifies the number of hidden layers to use in the decoder RNN\n",
        "    dropout: float\n",
        "        Defines the dropout rate for the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, static_size, dynamic_size, hidden_size,\n",
        "                 update_fn=None, mask_fn=None, num_layers=1, dropout=0.):\n",
        "        super(DRL4TSP, self).__init__()\n",
        "\n",
        "        if dynamic_size < 1:\n",
        "            raise ValueError(':param dynamic_size: must be > 0, even if the '\n",
        "                             'problem has no dynamic elements')\n",
        "\n",
        "        self.update_fn = update_fn\n",
        "        self.mask_fn = mask_fn\n",
        "\n",
        "        # Define the encoder & decoder models\n",
        "        self.static_encoder = Encoder(static_size, hidden_size)\n",
        "        self.dynamic_encoder = Encoder(dynamic_size, hidden_size)\n",
        "        self.decoder = Encoder(static_size, hidden_size)\n",
        "        self.pointer = Pointer(hidden_size, num_layers, dropout)\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if len(p.shape) > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "\n",
        "        # Used as a proxy initial state in the decoder when not specified\n",
        "        self.x0 = torch.zeros((1, static_size, 1), requires_grad=True, device=device)\n",
        "\n",
        "    def forward(self, static, dynamic, decoder_input=None, last_hh=None):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        static: Array of size (batch_size, feats, num_cities)\n",
        "            Defines the elements to consider as static. For the TSP, this could be\n",
        "            things like the (x, y) coordinates, which won't change\n",
        "        dynamic: Array of size (batch_size, feats, num_cities)\n",
        "            Defines the elements to consider as static. For the VRP, this can be\n",
        "            things like the (load, demand) of each city. If there are no dynamic\n",
        "            elements, this can be set to None\n",
        "        decoder_input: Array of size (batch_size, num_feats)\n",
        "            Defines the outputs for the decoder. Currently, we just use the\n",
        "            static elements (e.g. (x, y) coordinates), but this can technically\n",
        "            be other things as well\n",
        "        last_hh: Array of size (batch_size, num_hidden)\n",
        "            Defines the last hidden state for the RNN\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size, input_size, sequence_size = static.size()\n",
        "\n",
        "        if decoder_input is None:\n",
        "            decoder_input = self.x0.expand(batch_size, -1, -1)\n",
        "\n",
        "        # Always use a mask - if no function is provided, we don't update it\n",
        "        mask = torch.ones(batch_size, sequence_size, device=device)\n",
        "\n",
        "        # Structures for holding the output sequences\n",
        "        tour_idx, tour_logp = [], []\n",
        "        max_steps = sequence_size if self.mask_fn is None else 1000\n",
        "\n",
        "        # Static elements only need to be processed once, and can be used across\n",
        "        # all 'pointing' iterations. When / if the dynamic elements change,\n",
        "        # their representations will need to get calculated again.\n",
        "        static_hidden = self.static_encoder(static)\n",
        "        dynamic_hidden = self.dynamic_encoder(dynamic)\n",
        "\n",
        "        for _ in range(max_steps):\n",
        "\n",
        "            if not mask.byte().any():\n",
        "                break\n",
        "\n",
        "            # ... but compute a hidden rep for each element added to sequence\n",
        "            decoder_hidden = self.decoder(decoder_input)\n",
        "\n",
        "            probs, last_hh = self.pointer(static_hidden,\n",
        "                                          dynamic_hidden,\n",
        "                                          decoder_hidden, last_hh)\n",
        "            probs = F.softmax(probs + mask.log(), dim=1)\n",
        "\n",
        "            # When training, sample the next step according to its probability.\n",
        "            # During testing, we can take the greedy approach and choose highest\n",
        "            if self.training:\n",
        "                m = torch.distributions.Categorical(probs)\n",
        "\n",
        "                # Sometimes an issue with Categorical & sampling on GPU; See:\n",
        "                # https://github.com/pemami4911/neural-combinatorial-rl-pytorch/issues/5\n",
        "                ptr = m.sample()\n",
        "                while not torch.gather(mask, 1, ptr.data.unsqueeze(1)).byte().all():\n",
        "                    ptr = m.sample()\n",
        "                logp = m.log_prob(ptr)\n",
        "            else:\n",
        "                prob, ptr = torch.max(probs, 1)  # Greedy\n",
        "                logp = prob.log()\n",
        "\n",
        "            # After visiting a node update the dynamic representation\n",
        "            if self.update_fn is not None:\n",
        "                dynamic = self.update_fn(dynamic, ptr.data)\n",
        "                dynamic_hidden = self.dynamic_encoder(dynamic)\n",
        "\n",
        "                # Since we compute the VRP in minibatches, some tours may have\n",
        "                # number of stops. We force the vehicles to remain at the depot\n",
        "                # in these cases, and logp := 0\n",
        "                is_done = dynamic[:, 1].sum(1).eq(0).float()\n",
        "                logp = logp * (1. - is_done)\n",
        "\n",
        "            # And update the mask so we don't re-visit if we don't need to\n",
        "            if self.mask_fn is not None:\n",
        "                mask = self.mask_fn(mask, dynamic, ptr.data).detach()\n",
        "\n",
        "            tour_logp.append(logp.unsqueeze(1))\n",
        "            tour_idx.append(ptr.data.unsqueeze(1))\n",
        "\n",
        "            decoder_input = torch.gather(static, 2,\n",
        "                                         ptr.view(-1, 1, 1)\n",
        "                                         .expand(-1, input_size, 1)).detach()\n",
        "\n",
        "        tour_idx = torch.cat(tour_idx, dim=1)  # (batch_size, seq_len)\n",
        "        tour_logp = torch.cat(tour_logp, dim=1)  # (batch_size, seq_len)\n",
        "\n",
        "        return tour_idx, tour_logp\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    raise Exception('Cannot be called from main')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "xYNyGmsyU_J9",
        "outputId": "b3f9311b-0729-4acd-ecd4-e89982319fbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\QUKAIC~1\\AppData\\Local\\Temp/ipykernel_6692/2675666365.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot be called from main'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mException\u001b[0m: Cannot be called from main"
          ]
        }
      ]
    }
  ]
}